{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat by using ollama api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display, Markdown\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a system message list\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Describe a full cycle of the Recruitment process\"},\n",
    "]\n",
    "\n",
    "# create a request payload\n",
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python Ollama package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage Content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Python library 0.4 with function calling improvements Â· Ollama Blog\n"
     ]
    }
   ],
   "source": [
    "webcontent = Website(\"https://ollama.com/blog/functions-as-tools\")\n",
    "print(webcontent.title)\n",
    "# print(webcontent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}. \\\n",
    "    The contents of this website is as follows; \\\n",
    "    Please provide a short summary of this website in markdown. \\\n",
    "    If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    \n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# How the Best Chief Data Officers Create Value\n",
      "\n",
      "## Summary\n",
      "\n",
      "The majority of chief data officers (CDOs) fail to value and price the business outcomes created by their data and analytics capabilities, leading to short tenures. However, based on 17 in-depth interviews with CDOs who are considered at the frontier of the role, researchers Suraj Srinivasan and Robin Seibert identified four spheres of influence where CDOs can create value: \n",
      "\n",
      "1. **Strategic Leadership**: Emphasizing alignment with business goals and establishing clear metrics for success.\n",
      "2. **Data Governance**: Developing data management capabilities, ensuring data quality, and implementing effective data governance frameworks.\n",
      "3. **Data Innovation**: Fostering a culture of innovation, leveraging AI and machine learning to drive insights, and integrating data into decision-making processes.\n",
      "4. **Organizational Alignment**: Ensuring that the organization is equipped to harness the power of data analytics capabilities.\n",
      "\n",
      "By focusing on these spheres of influence, CDOs can create value for their organizations and establish themselves as strategic business partners.\n",
      "\n",
      "## News/Announcements\n",
      "\n",
      "* Harvard Business Review has announced a new online leadership training course called Digital Intelligence.\n",
      "* The course aims to help professionals accelerate their careers in a rapidly transforming world by developing digital intelligence skills.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL = \"llama3.2\"\n",
    "ed = Website(\"https://hbr.org/2023/09/how-the-best-chief-data-officers-create-value\")\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages_for(ed))\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "class llms:\n",
    "    def __init__(self, model, company, website, url):\n",
    "        self.model = MODEL\n",
    "        self.website = website\n",
    "        self.company = company\n",
    "        self.url = url\n",
    "    \n",
    "        self.llm = ChatOllama(\n",
    "            model = self.model,\n",
    "            temperature = 0.8,\n",
    "            num_predict = 256,\n",
    "            # other params ...\n",
    "            )\n",
    "    \n",
    "    def get_relevant_links(self):\n",
    "        system_prompt = \"\"\"You are provided with a list of links found on a webpage.\\n \\\n",
    "        You should only include links that are relevant to the company brochure,\\n \\\n",
    "        such as links to an About page, or a Company page, or products, or solution pages.\\n\n",
    "        You should respond in JSON as the example below. I don't need you give any explanation, just a JSON response.\\n:\n",
    "            \"links\": [\n",
    "                {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "                {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "            ]\n",
    "        \"\"\"\n",
    "    \n",
    "        user_prompt = f\"\"\"Here is the company website of {self.url}.\n",
    "            I will provide you with a list of links found on the website as below: {self.website.get_links(self.url)} \\n\n",
    "            Please decide which of these are relevant web links for a company brochure, respond with the full https URL in JSON format.\n",
    "            Step by step review the solutions or products and include them the relevant links.\n",
    "            Do not include Terms of Service, Privacy, email links.\"\"\"\n",
    "\n",
    "        payload = [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", user_prompt),\n",
    "            ]\n",
    "\n",
    "        response = self.llm.invoke(input=payload)\n",
    "        # print(response)\n",
    "        # result = response[\"content\"]\n",
    "        return response\n",
    "\n",
    "    def get_all_details(self):\n",
    "        relevant_links = self.get_relevant_links()\n",
    "        # print(relevant_links)\n",
    "\n",
    "        details = []\n",
    "        for link in relevant_links[\"links\"]:\n",
    "            content = self.website.get_content(link[\"url\"])\n",
    "            details.append({\"type\": link[\"type\"], \"content\": content})\n",
    "        return details\n",
    "\n",
    "    def get_brochure_user_prompt(company_name):\n",
    "        user_prompt = f\"\"\"You are looking at a company called: {self.company_name}\\n\n",
    "        Use the below information to build a short brochure of the company in markdown.\\n\n",
    "        The company's website is: {self.website.url}\\n\n",
    "        The wesite's contents are: {self.get_all_details()}\"\"\"\n",
    "        return user_prompt\n",
    "\n",
    "\n",
    "class Website:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "            # Add more headers as needed\n",
    "        }\n",
    "\n",
    "    def get_raw_html(self, url):\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        body = response.content\n",
    "        soup = BeautifulSoup(body, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    def get_title(self, url):\n",
    "        soup = self.get_raw_html(url)\n",
    "        return soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "    def get_content(self, url):\n",
    "        soup = self.get_raw_html(url)\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            text = \"\"\n",
    "        return text\n",
    "\n",
    "    def get_links(self, url):\n",
    "        soup = self.get_raw_html(url)\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "        return f\"\\n\".join(self.links) + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOllama â€” ðŸ¦œðŸ”— LangChain  documentation\n",
      "#main-content\n",
      "../../index.html\n",
      "../../reference.html\n",
      "https://python.langchain.com/\n",
      "https://github.com/langchain-ai/langchain\n",
      "https://twitter.com/langchainai\n",
      "../../reference.html\n",
      "https://python.langchain.com/\n",
      "https://github.com/langchain-ai/langchain\n",
      "https://twitter.com/langchainai\n",
      "../../core/index.html\n",
      "../../langchain/index.html\n",
      "../../text_splitters/index.html\n",
      "../../community/index.html\n",
      "../../experimental/index.html\n",
      "../../ai21/index.html\n",
      "../../anthropic/index.html\n",
      "../../astradb/index.html\n",
      "../../aws/index.html\n",
      "../../azure_dynamic_sessions/index.html\n",
      "../../box/index.html\n",
      "../../cerebras/index.html\n",
      "../../chroma/index.html\n",
      "../../cohere/index.html\n",
      "../../couchbase/index.html\n",
      "../../databricks/index.html\n",
      "../../elasticsearch/index.html\n",
      "../../exa/index.html\n",
      "../../fireworks/index.html\n",
      "../../google_community/index.html\n",
      "../../google_genai/index.html\n",
      "../../google_vertexai/index.html\n",
      "../../groq/index.html\n",
      "../../huggingface/index.html\n",
      "../../ibm/index.html\n",
      "../../milvus/index.html\n",
      "../../mistralai/index.html\n",
      "../../neo4j/index.html\n",
      "../../nomic/index.html\n",
      "../../nvidia_ai_endpoints/index.html\n",
      "../index.html\n",
      "../chat_models.html\n",
      "#\n",
      "../embeddings.html\n",
      "../llms.html\n",
      "../../openai/index.html\n",
      "../../pinecone/index.html\n",
      "../../postgres/index.html\n",
      "../../prompty/index.html\n",
      "../../qdrant/index.html\n",
      "../../redis/index.html\n",
      "../../sema4/index.html\n",
      "../../snowflake/index.html\n",
      "../../sqlserver/index.html\n",
      "../../standard_tests/index.html\n",
      "../../together/index.html\n",
      "../../unstructured/index.html\n",
      "../../upstage/index.html\n",
      "../../voyageai/index.html\n",
      "../../weaviate/index.html\n",
      "../../xai/index.html\n",
      "../../index.html\n",
      "../../reference.html\n",
      "../index.html\n",
      "../chat_models.html\n",
      "#chatollama\n",
      "../../_modules/langchain_ollama/chat_models.html#ChatOllama\n",
      "#langchain_ollama.chat_models.ChatOllama\n",
      "../../core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_retry\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.assign\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_graph\n",
      "#langchain_ollama.chat_models.ChatOllama.base_url\n",
      "../../core/caches/langchain_core.caches.BaseCache.html#langchain_core.caches.BaseCache\n",
      "#langchain_ollama.chat_models.ChatOllama.cache\n",
      "../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager\n",
      "#langchain_ollama.chat_models.ChatOllama.callback_manager\n",
      "#langchain_ollama.chat_models.ChatOllama.callbacks\n",
      "#langchain_ollama.chat_models.ChatOllama.callbacks\n",
      "#langchain_ollama.chat_models.ChatOllama.client_kwargs\n",
      "https://pydoc.dev/httpx/latest/httpx.Client.html\n",
      "#langchain_ollama.chat_models.ChatOllama.custom_get_token_ids\n",
      "#langchain_ollama.chat_models.ChatOllama.disable_streaming\n",
      "#langchain_ollama.chat_models.ChatOllama.format\n",
      "#langchain_ollama.chat_models.ChatOllama.keep_alive\n",
      "#langchain_ollama.chat_models.ChatOllama.metadata\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat_eta\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat_tau\n",
      "#langchain_ollama.chat_models.ChatOllama.model\n",
      "#langchain_ollama.chat_models.ChatOllama.num_ctx\n",
      "#langchain_ollama.chat_models.ChatOllama.num_gpu\n",
      "#langchain_ollama.chat_models.ChatOllama.num_predict\n",
      "#langchain_ollama.chat_models.ChatOllama.num_thread\n",
      "../../core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter\n",
      "#langchain_ollama.chat_models.ChatOllama.rate_limiter\n",
      "#langchain_ollama.chat_models.ChatOllama.repeat_last_n\n",
      "#langchain_ollama.chat_models.ChatOllama.repeat_penalty\n",
      "#langchain_ollama.chat_models.ChatOllama.seed\n",
      "#langchain_ollama.chat_models.ChatOllama.stop\n",
      "#langchain_ollama.chat_models.ChatOllama.tags\n",
      "#langchain_ollama.chat_models.ChatOllama.temperature\n",
      "#langchain_ollama.chat_models.ChatOllama.tfs_z\n",
      "#langchain_ollama.chat_models.ChatOllama.top_k\n",
      "#langchain_ollama.chat_models.ChatOllama.top_p\n",
      "#langchain_ollama.chat_models.ChatOllama.verbose\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler\n",
      "../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "#langchain_ollama.chat_models.ChatOllama.__call__\n",
      "#langchain_ollama.chat_models.ChatOllama.invoke\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler\n",
      "../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "#langchain_ollama.chat_models.ChatOllama.abatch\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "#langchain_ollama.chat_models.ChatOllama.abatch_as_completed\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "#langchain_ollama.chat_models.ChatOllama.ainvoke\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk\n",
      "#langchain_ollama.chat_models.ChatOllama.astream\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.schema.StandardStreamEvent.html#langchain_core.runnables.schema.StandardStreamEvent\n",
      "../../core/runnables/langchain_core.runnables.schema.CustomStreamEvent.html#langchain_core.runnables.schema.CustomStreamEvent\n",
      "#langchain_ollama.chat_models.ChatOllama.astream_events\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.schema.StandardStreamEvent.html#langchain_core.runnables.schema.StandardStreamEvent\n",
      "../../core/runnables/langchain_core.runnables.schema.CustomStreamEvent.html#langchain_core.runnables.schema.CustomStreamEvent\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "#langchain_ollama.chat_models.ChatOllama.batch\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "#langchain_ollama.chat_models.ChatOllama.batch_as_completed\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.bind\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../_modules/langchain_ollama/chat_models.html#ChatOllama.bind_tools\n",
      "#langchain_ollama.chat_models.ChatOllama.bind_tools\n",
      "../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool\n",
      "../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable\n",
      "#langchain_ollama.chat_models.ChatOllama.configurable_alternatives\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption\n",
      "../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable\n",
      "#langchain_ollama.chat_models.ChatOllama.configurable_fields\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption\n",
      "../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption\n",
      "../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable\n",
      "#langchain_ollama.chat_models.ChatOllama.get_num_tokens\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "#langchain_ollama.chat_models.ChatOllama.get_num_tokens_from_messages\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "#langchain_ollama.chat_models.ChatOllama.get_token_ids\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "#langchain_ollama.chat_models.ChatOllama.invoke\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk\n",
      "#langchain_ollama.chat_models.ChatOllama.stream\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_alisteners\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_config\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_fallbacks\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_listeners\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_retry\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../_modules/langchain_ollama/chat_models.html#ChatOllama.with_structured_output\n",
      "#langchain_ollama.chat_models.ChatOllama.with_structured_output\n",
      "../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool\n",
      "https://ollama.com/blog/structured-outputs\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue\n",
      "../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama.with_types\n",
      "../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable\n",
      "#langchain_ollama.chat_models.ChatOllama\n",
      "#langchain_ollama.chat_models.ChatOllama.base_url\n",
      "#langchain_ollama.chat_models.ChatOllama.cache\n",
      "#langchain_ollama.chat_models.ChatOllama.callback_manager\n",
      "#langchain_ollama.chat_models.ChatOllama.callbacks\n",
      "#langchain_ollama.chat_models.ChatOllama.client_kwargs\n",
      "#langchain_ollama.chat_models.ChatOllama.custom_get_token_ids\n",
      "#langchain_ollama.chat_models.ChatOllama.disable_streaming\n",
      "#langchain_ollama.chat_models.ChatOllama.format\n",
      "#langchain_ollama.chat_models.ChatOllama.keep_alive\n",
      "#langchain_ollama.chat_models.ChatOllama.metadata\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat_eta\n",
      "#langchain_ollama.chat_models.ChatOllama.mirostat_tau\n",
      "#langchain_ollama.chat_models.ChatOllama.model\n",
      "#langchain_ollama.chat_models.ChatOllama.num_ctx\n",
      "#langchain_ollama.chat_models.ChatOllama.num_gpu\n",
      "#langchain_ollama.chat_models.ChatOllama.num_predict\n",
      "#langchain_ollama.chat_models.ChatOllama.num_thread\n",
      "#langchain_ollama.chat_models.ChatOllama.rate_limiter\n",
      "#langchain_ollama.chat_models.ChatOllama.repeat_last_n\n",
      "#langchain_ollama.chat_models.ChatOllama.repeat_penalty\n",
      "#langchain_ollama.chat_models.ChatOllama.seed\n",
      "#langchain_ollama.chat_models.ChatOllama.stop\n",
      "#langchain_ollama.chat_models.ChatOllama.tags\n",
      "#langchain_ollama.chat_models.ChatOllama.temperature\n",
      "#langchain_ollama.chat_models.ChatOllama.tfs_z\n",
      "#langchain_ollama.chat_models.ChatOllama.top_k\n",
      "#langchain_ollama.chat_models.ChatOllama.top_p\n",
      "#langchain_ollama.chat_models.ChatOllama.verbose\n",
      "#langchain_ollama.chat_models.ChatOllama.__call__\n",
      "#langchain_ollama.chat_models.ChatOllama.abatch\n",
      "#langchain_ollama.chat_models.ChatOllama.abatch_as_completed\n",
      "#langchain_ollama.chat_models.ChatOllama.ainvoke\n",
      "#langchain_ollama.chat_models.ChatOllama.astream\n",
      "#langchain_ollama.chat_models.ChatOllama.astream_events\n",
      "#langchain_ollama.chat_models.ChatOllama.batch\n",
      "#langchain_ollama.chat_models.ChatOllama.batch_as_completed\n",
      "#langchain_ollama.chat_models.ChatOllama.bind\n",
      "#langchain_ollama.chat_models.ChatOllama.bind_tools\n",
      "#langchain_ollama.chat_models.ChatOllama.configurable_alternatives\n",
      "#langchain_ollama.chat_models.ChatOllama.configurable_fields\n",
      "#langchain_ollama.chat_models.ChatOllama.get_num_tokens\n",
      "#langchain_ollama.chat_models.ChatOllama.get_num_tokens_from_messages\n",
      "#langchain_ollama.chat_models.ChatOllama.get_token_ids\n",
      "#langchain_ollama.chat_models.ChatOllama.invoke\n",
      "#langchain_ollama.chat_models.ChatOllama.stream\n",
      "#langchain_ollama.chat_models.ChatOllama.with_alisteners\n",
      "#langchain_ollama.chat_models.ChatOllama.with_config\n",
      "#langchain_ollama.chat_models.ChatOllama.with_fallbacks\n",
      "#langchain_ollama.chat_models.ChatOllama.with_listeners\n",
      "#langchain_ollama.chat_models.ChatOllama.with_retry\n",
      "#langchain_ollama.chat_models.ChatOllama.with_structured_output\n",
      "#langchain_ollama.chat_models.ChatOllama.with_types\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html\"\n",
    "website_crape = Website()\n",
    "print(website_crape.get_title(url))\n",
    "# print(website_crape.get_content(url))\n",
    "print(website_crape.get_links(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_instance = llms(model=MODEL, company='Langchain',website=website_crape, url=url)\n",
    "response = llms_instance.get_relevant_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the list of relevant web links in JSON format:\\n\\n```\\n{\\n  \"links\": [\\n    {\\n      \"title\": \"Chat OLLAMA Documentation\",\\n      \"url\": \"https://ollama.readthedocs.io/en/latest/index.html\"\\n    },\\n    {\\n      \"title\": \"OLLA-Ma - AI-powered Conversational Interface\",\\n      \"url\": \"https://ollama.readthedocs.io/en/latest/ollematools.html\"\\n    },\\n    {\\n      \"title\": \"Chat OLLAMA Model Hub\",\\n      \"url\": \"https://huggingface.co/models?query=OLLAMA\"\\n    },\\n    {\\n      \"title\": \"OLLA-Ma GitHub Repository\",\\n      \"url\": \"https://github.com/ollama/ollematools\"\\n    }\\n  ]\\n}\\n```\\n\\nThese links are relevant to the Chat OLLAMA product and its related solutions:\\n\\n* The documentation link provides an overview of the chat interface and its features.\\n* The OLLA-Ma link explains the AI-powered conversational interface that powers the chat interface.\\n* The model hub link allows users to explore and access pre-trained models for the chat interface.\\n* The GitHub repository link provides access to the codebase and development tools for the product'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = response.content\n",
    "rs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
