{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face token\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"\"\"Is there any way that we can speed up it? \n",
    "                    Not unless you can help me fix the issue?\n",
    "                    NO, I CANNOT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Sentiment\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out = classifier(text)\n",
    "df = pd.DataFrame(classifier(text))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "#NER - Named Entity Recognition\n",
    "ner = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "out = ner(\"Ho Chi Minh is the name of a city in Vietnam\")\n",
    "pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = \"\"\"\n",
    "Oversee the translation of product features, carbon accounting standards, and related data sets into great user experience, scalable data models, data architectures, and API functions. \n",
    "Provide technical oversight and leadership to the Engineering and AI & Machine Learning teams especially utilizing Go, Python, Amazon Web Services, MySQL, Snowflake, and Databricks. \n",
    "Deliver according to prioritized feature, integration and analytical data platform roadmaps for both Persefoni SaaS products and supporting tools and features. \n",
    "Build, secure, and document a highly usable, scalable, and reliable public API for those customers wanting to utilize Persefoni’s capabilities without the assistance of the Persefoni front end interface. \n",
    "Collaborate with a global team of Software Engineers (front end), Product Management (product leadership), Quality Assurance (testing), Cloud Architecture, and DevSecOps to deliver Persefoni’s Climate Management & Accounting Platform with increasing speed and quality. \n",
    "Help scale the existing Vietnam Engineering team while ensuring engineering and delivery maturity, and strategic alignment with business objectives. \n",
    "Attract, onboard, lead, develop, and grow technical engineering talent. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversee the translation of product features, carbon accounting standards, and related data sets into great user experience . provide technical oversight and leadership to the Engineering and AI & Machine Learning teams especially utilizing Go, Python, Amazon Web Services, MySQL, Snowflake, and Databricks .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "model = pipeline(\"summarization\")\n",
    "text = jd_text\n",
    "try:\n",
    "    out = model(text)\n",
    "    print(out[0]['summary_text'])  # Access the summary text from the output\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversee the translation of product features, carbon accounting standards, and related data sets into great user experience . provide technical oversight and leadership to the Engineering and AI & Machine Learning teams especially utilizing Go, Python, Amazon Web Services, MySQL, Snowflake, and Databricks .\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['summary_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
